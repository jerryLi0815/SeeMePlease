{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e548775-e03a-4af7-a8a1-1a8b2a435324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install pyspellchecker\n",
    "!pip install torch==2.1.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.43.1\n",
    "!pip install accelerate\n",
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd9e191-34fa-46e8-9b50-aa94a776a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:01<00:00, 56.3MiB/s]\n",
      "ffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 8 (Debian 8.3.0-6)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/workspace/audio/2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.m4a':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-09-09T23:28:23.000000Z\n",
      "  Duration: 00:06:59.07, start: 0.000000, bitrate: 67 kb/s\n",
      "  Stream #0:0[0x1](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 66 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-09-09T23:28:23.000000Z\n",
      "        handler_name    : AAC audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/workspace/audio/2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-09-09T23:28:23.000000Z\n",
      "        handler_name    : AAC audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "[out#0/wav @ 0x84f3e00] video:0KiB audio:13096KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000582%\n",
      "size=   13096KiB time=00:06:59.07 bitrate= 256.0kbits/s speed=1.16e+03x    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /workspace/audio/2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.m4a to /workspace/audio/2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.wav\n",
      "Transcription saved to /workspace/data/2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.txt\n",
      "Text has been processed and saved to /workspace/clean_data/processed_2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.txt.\n",
      "All audio files have been transcribed and processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import warnings\n",
    "import re\n",
    "import contractions\n",
    "import difflib\n",
    "from spellchecker import SpellChecker\n",
    "from difflib import SequenceMatcher\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "audio_directory = \"/workspace/audio\"\n",
    "transcription_directory = \"/workspace/data\"\n",
    "processed_directory = \"/workspace/clean_data\"\n",
    "\n",
    "os.makedirs(transcription_directory, exist_ok=True)\n",
    "os.makedirs(processed_directory, exist_ok=True)\n",
    "\n",
    "# os.environ['PATH'] = '/home/linuxbrew/.linuxbrew/opt/ffmpeg/bin:' + os.environ['PATH']\n",
    "\n",
    "def convert_audio_to_wav(input_file, output_file):\n",
    "    # ffmpeg_command = f\"/usr/local/bin/ffmpeg -y -i {input_file} -ar 16000 -ac 1 {output_file}\"\n",
    "    ffmpeg_command = f'/usr/local/bin/ffmpeg -y -i \"{input_file}\" -ar 16000 -ac 1 \"{output_file}\"'\n",
    "\n",
    "    subprocess.run(ffmpeg_command, shell=True, check=True)\n",
    "    print(f\"Converted {input_file} to {output_file}\")\n",
    "\n",
    "\n",
    "def transcribe_audio_with_whisper(audio_file, output_text_file):\n",
    "    result = model.transcribe(audio_file)\n",
    "    transcription_text = result['text']\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(transcription_text)\n",
    "    print(f\"Transcription saved to {output_text_file}\")\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def remove_greetings(text):\n",
    "    greetings_to_remove = [\n",
    "        'hello everyone', 'thank you and goodbye', 'thank you for visiting',\n",
    "        'dear sir', 'dear madam', 'sincerely yours', 'best regards',\n",
    "        'sorry', 'hi', 'hello', 'thanks', 'thank you', 'goodbye', 'happy birthday', 'yeah', 'um'\n",
    "    ]\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, greetings_to_remove)) + r')\\b'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_embedded_fillers(text):\n",
    "    fillers = [r'\\b(?:um|ah|uh|erm|hmm|you know|so|like|actually|basically|literally)\\b']\n",
    "    return re.sub('|'.join(fillers), '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,!?;:\\'-]', '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "def remove_number_sequences(text):\n",
    "    pattern = r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten)(?:\\s(?:zero|one|two|three|four|five|six|seven|eight|nine|ten)){1,}\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_repeated_phrases(text, n=5):\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if i + n > len(words):\n",
    "            cleaned_words.extend(words[i:])\n",
    "            break\n",
    "        current_phrase = ' '.join(words[i:i+n])\n",
    "        if current_phrase in ' '.join(words[i+n:]):\n",
    "            i += n\n",
    "        else:\n",
    "            cleaned_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "def remove_similar_phrases(text, similarity_threshold=0.8):\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        found_similar = False\n",
    "        for j in range(min(len(words) - i, 10), 0, -1):\n",
    "            current_phrase = ' '.join(words[i:i + j])\n",
    "            next_phrase = ' '.join(words[i + j:i + 2 * j])\n",
    "            if SequenceMatcher(None, current_phrase, next_phrase).ratio() >= similarity_threshold:\n",
    "                i += j\n",
    "                found_similar = True\n",
    "                break\n",
    "        if not found_similar:\n",
    "            cleaned_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "def remove_repeated_words(text):\n",
    "    words = text.split()\n",
    "    return ' '.join(word for i, word in enumerate(words) if i == 0 or word != words[i-1])\n",
    "\n",
    "def correct_spelling(text):\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    return ' '.join(spell.correction(word) or word for word in words)\n",
    "\n",
    "def split_by_uppercase_I(text):\n",
    "    split_text = re.split(r'(?<=\\s)(?=I\\b)', text)\n",
    "    return split_text\n",
    "\n",
    "def process_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = remove_greetings(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_embedded_fillers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_number_sequences(text)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    text = remove_repeated_phrases(text)\n",
    "    text = remove_similar_phrases(text)\n",
    "    text = remove_repeated_words(text)\n",
    "    text = correct_spelling(text)\n",
    "    split_text = split_by_uppercase_I(text)\n",
    "    return '\\n'.join(split_text)\n",
    "\n",
    "def process_file(input_file_path, output_file_path):\n",
    "    raw_text = read_text(input_file_path)\n",
    "    cleaned_text = process_text(raw_text)\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "    print(f\"Text has been processed and saved to {output_file_path}.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for filename in os.listdir(audio_directory):\n",
    "        if filename.endswith(\".m4a\") or filename.endswith(\".mp4\"):\n",
    "            audio_file_path = os.path.join(audio_directory, filename)\n",
    "            wav_file_path = os.path.join(audio_directory, f\"{os.path.splitext(filename)[0]}.wav\")\n",
    "            convert_audio_to_wav(audio_file_path, wav_file_path)\n",
    "            output_text_file_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_text_file_path = os.path.join(transcription_directory, output_text_file_name)\n",
    "            transcribe_audio_with_whisper(wav_file_path, output_text_file_path)\n",
    "\n",
    "    for filename in os.listdir(transcription_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(transcription_directory, filename)\n",
    "            output_file_path = os.path.join(processed_directory, f\"processed_{filename}\")\n",
    "            process_file(input_file_path, output_file_path)\n",
    "\n",
    "    print(\"All audio files have been transcribed and processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaf9c4e-5e37-4492-b3d8-ed179ef7fa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469edc5e601e4436a5f4ad1391073708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a668e4968f24ba2be90a9af4fbe8321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abe084ed0074429900f18f0f4ee2be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841805c642c947a2b658e25ba77ae476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0381e42162f40cb94cb18daa0a051ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dad704420244319266d9ca77638e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5351d6f0cba148aba15009b88d2af76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987dfd6a35f049a79c94d26b68a39477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117cb974f6744c889694674c74420494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedd8188e26e416496cb578b428d4b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e14ce70942f4f2d9274544f9650f924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0559e5291004089896ca5088267964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "folder_path = \"/workspace/models\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    token=\"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\",  \n",
    "    cache_dir=folder_path,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    token=\"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\", \n",
    "    cache_dir=folder_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46611cd-9d82-4fa9-a886-d9c4bcb33bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m145.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (69.0.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.67.1 markdown-3.7 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 werkzeug-3.0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.2+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.43.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.1 requires torch==2.1.1, but you have torch 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-18.0.0 requests-2.32.3 xxhash-3.5.0 yarl-1.17.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tensorboard\n",
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8a7575-7109-483d-9a92-45d9313862e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f09f213-4ab8-4841-9b65-5e3faa209c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bacef45f39f490c9fc3accf4f6c7317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392a40a224a34650a18af86f7038ccf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 07:46, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0.968421052631579: Training loss = 0.3792\n",
      "\n",
      "Epoch 1.9789473684210526: Training loss = 0.2193\n",
      "\n",
      "Epoch 2.905263157894737: Training loss = 0.1575\n",
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, AdamW, get_constant_schedule, ProgressCallback\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainerCallback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nEpoch {state.epoch}: Training loss = {state.log_history[-1]['loss']}\")\n",
    "\n",
    "def process_func(example):\n",
    "    # print(example)\n",
    "    MAX_LENGTH = 1024\n",
    "    instruction_text = (\n",
    "        f\"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"You are an emotional and behavior analyst. Your task is to analyze the behaviors \"\n",
    "        f\"and emotions in the input content, ensuring a comprehensive context consideration. \"\n",
    "        f\"Follow these steps:\\n\"\n",
    "        f\"1. Identify key actions, themes, or behaviors contributing to the emotions in the input content.\\n\"\n",
    "        f\"2. For each identified theme, analyze its context and viewpoint.\\n\"\n",
    "        f\"3. Determine the emotional polarity (positive/negative) for each theme. Reclassify neutral emotions as negative.\\n\"\n",
    "        f\"4. Directly classify statements describing problems, frustrations, or difficulties as negative. Conversely, classify descriptions of achievements, success, or ease as positive.\\n\"\n",
    "        f\"5. Provide reasons for each identified behavior that justify the emotional classification.\\n\"\n",
    "        f\"6. Omit any irrelevant or nonsensical content that lacks a clear viewpoint or emotional polarity.\\n\"\n",
    "        f\"Example:\\n\"\n",
    "        f\"- behavior: User clicks the 'Submit' button but it does not respond.\\n\"\n",
    "        f\"- emotional: Negative\\n\"\n",
    "        f\"- reason: User feels frustrated as they are unable to complete their action.\\n\\n\"\n",
    "        f\"Now analyze the input content in a similar way:\\n\\n\"\n",
    "        f\"Input content: {example['context']}\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"- behavior: [First identified behavior in detail]\\n\"\n",
    "        f\"- emotional: [Positive/Negative]\\n\"\n",
    "        f\"- reason: [Reason explaining the emotional classification for the first behavior]\\n\"\n",
    "        f\"- behavior: [Second identified behavior in detail]\\n\"\n",
    "        f\"- emotional: [Positive/Negative]\\n\"\n",
    "        f\"- reason: [Reason explaining the emotional classification for the second behavior]\\n\"\n",
    "        f\"... (continue for all identified behaviors, emotions, and reasons)\\n\"\n",
    "        f\"<|eot_id|>\"\n",
    "    ).strip()\n",
    "\n",
    "    output = (\n",
    "        f\"behavior: {example['behavior']}\\n\"\n",
    "        f\"emotional: {example['label']}\\n\"\n",
    "        f\"reason: {example['reason']}\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "    instruction = tokenizer(instruction_text, add_special_tokens=False)\n",
    "    response = tokenizer(output, add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  \n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    writer = SummaryWriter()\n",
    "    file_path = '/workspace/annotation.txt'\n",
    "    data_train = pd.read_csv(file_path, sep='####', header=None, engine='python')\n",
    "    data_train.columns = ['context', 'behavior', 'reason', 'label']\n",
    "    # print(data_train.head())\n",
    "    data_train = Dataset.from_pandas(data_train)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"/workspace/models/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\", use_fast=False,trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"/workspace/models/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\",\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        # device_map=\"auto\",\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    train_tokenized = data_train.map(process_func, remove_columns=data_train.column_names)\n",
    "    # setup peft\n",
    "    config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    inference_mode=False, \n",
    "    r=16, \n",
    "    lora_alpha=64, \n",
    "    lora_dropout=0.1 \n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    model.print_trainable_parameters()\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./Llama\",\n",
    "        per_device_train_batch_size=1,\n",
    "        # per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        do_train=True,\n",
    "        # do_eval=True,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=50,\n",
    "        learning_rate=1e-4,\n",
    "        # save_on_each_node=True,\n",
    "        gradient_checkpointing=True,\n",
    "        # bf16=True, \n",
    "        metric_for_best_model='loss',\n",
    "        # weight_decay=0.001,\n",
    "        # evaluation_strategy=\"steps\",\n",
    "        # eval_steps=50,\n",
    "        # load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        callbacks=[PrintLossCallback()],\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "    )\n",
    "    trainer.train()\n",
    "    peft_model_id = \"./Llama_lora\"\n",
    "    trainer.model.save_pretrained(peft_model_id)\n",
    "    tokenizer.save_pretrained(peft_model_id)\n",
    "    print(\"Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2767b6-5669-45c9-98ea-1b2542348450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e5265332a442ea9461f0cef2b9f034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: All right\n",
      "behavior: All right\n",
      "emotional: Postive\n",
      "reason: right\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am on the serves Australia linked in page now\n",
      "behavior: on the serves Australia linked in page\n",
      "emotional: Postive\n",
      "reason: now\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am just going to have a little look See what the headings look Let us see Let us number one No alternative text there is no description on any of the image there which sucks because it is pretty important\n",
      "behavior: it is pretty important\n",
      "emotional: Negative\n",
      "reason: there is no description on any of the image\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I guess that what the image is\n",
      "behavior: what the image is\n",
      "emotional: Negative\n",
      "reason: guess\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I think not sure what it is okay look it is great because there is a great description above with the actual post itself but it is not describing what is in the actual image which is , there is one Particularly with media linked where it is something this is it is heavily important that\n",
      "behavior: it is not describing what is in the actual image\n",
      "emotional: Negative\n",
      "reason: it is great because there is a great description above\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I guess all the image is one of the main things that is post it is that information it is really important that everybody gets that information It is not about where we are purple day that is cool But there is no all text\n",
      "behavior: there is no all text\n",
      "emotional: Negative\n",
      "reason: it is really important that everybody gets that information\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I do not want to be a broken record but all text is really important Does not need to be huge thing It is just a little description of what is in the image and then everybody's got the same information which is really important to make everybody feel included alright . Just going to try and look through this And now for page one okay that down you can hear what that says is what it is trying to say\n",
      "behavior: all text is really important\n",
      "emotional: Postive\n",
      "reason: make everybody feel included\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I think it is reading at this heading But that is it is reading at maybe what the coding is on the bottom\n",
      "behavior: reading at this heading\n",
      "emotional: Negative\n",
      "reason: But that is it is reading at maybe what the coding is on the bottom\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I do not understand\n",
      "behavior: understand\n",
      "emotional: Negative\n",
      "reason: do not\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I know of is a nightmare with screen readers That is it is pretty much all\n",
      "behavior: screen readers\n",
      "emotional: Negative\n",
      "reason: is a nightmare\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I have to say\n",
      "behavior: say\n",
      "emotional: Negative\n",
      "reason: have to\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I can access the information sometimes but it reads out hundreds of things that are not even there It does not read out anything correctly a lot of the time\n",
      "behavior: it reads out hundreds of things that are not even there\n",
      "emotional: Negative\n",
      "reason: I can access the information sometimes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am going to slow that down right down and just this is what\n",
      "behavior: going to slow\n",
      "emotional: Negative\n",
      "reason: down\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am hearing it is equals caught What does that even mean That is bizarre It just said the same thing again You are currently on a text elements It is not even reading any of the text on the screen pass\n",
      "behavior: it is equals caught\n",
      "emotional: Negative\n",
      "reason: bizarre\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I think they can be done to be accessible but more often than not pass are just a nightmare When it comes to a screen reader and not even just a screen reader if you are using magnification and things that They can just be a bit tough to navigate It is not a lot of things\n",
      "behavior: pass are just a nightmare\n",
      "emotional: Negative\n",
      "reason: using magnification and things\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I can be done a lot better and easier Now it is just given up on me It is just keeps on reading that same thing over and over again\n",
      "behavior: given up on me\n",
      "emotional: Negative\n",
      "reason: done a lot better and easier\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I do not even know what it means\n",
      "behavior: know\n",
      "emotional: Negative\n",
      "reason: do not even\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I would say whatever the coding is or whatever that headings under the text It is providing the of image It is just reading that part of it and not what is on the image\n",
      "behavior: providing the of image\n",
      "emotional: Negative\n",
      "reason: It is just reading that part of it and not what is on the image\n",
      "\n",
      "Input: I guess if it is a of if there is maybe an all text to a text box that has what it says or something that There is ways of fixing it but it is just a matter of labeling it and whatnot All right well hopefully this was helpful .\n",
      "behavior: a matter of labeling it and whatnot\n",
      "emotional: Negative\n",
      "reason: there is ways of fixing it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "mode_path = '/workspace/Llama_lora'\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "def generate_answer(example):\n",
    "    prompt = (\n",
    "        f\"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"You are an emotional and behavior analyst. Your task is to analyze the behaviors \"\n",
    "        f\"and emotions in the input content, ensuring a comprehensive context consideration. \"\n",
    "        f\"Follow these steps:\\n\"\n",
    "        f\"1. Identify key actions, themes, or behaviors contributing to the emotions in the input content.\\n\"\n",
    "        f\"2. For each identified theme, analyze its context and viewpoint.\\n\"\n",
    "        f\"3. Determine the emotional polarity (positive/negative) for each theme. Reclassify neutral emotions as negative.\\n\"\n",
    "        f\"4. Directly classify statements describing problems, frustrations, or difficulties as negative. Conversely, classify descriptions of achievements, success, or ease as positive.\\n\"\n",
    "        f\"5. Provide reasons for each identified behavior that justify the emotional classification.\\n\"\n",
    "        f\"6. Omit any irrelevant or nonsensical content that lacks a clear viewpoint or emotional polarity.\\n\"\n",
    "        f\"Example:\\n\"\n",
    "        f\"- behavior: User clicks the 'Submit' button but it does not respond.\\n\"\n",
    "        f\"- emotional: Negative\\n\"\n",
    "        f\"- reason: User feels frustrated as they are unable to complete their action.\\n\\n\"\n",
    "        f\"Now analyze the input content in a similar way:\\n\\n\"\n",
    "        f\"Input content: {example}\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"- behavior: [First identified behavior in detail]\\n\"\n",
    "        f\"- emotional: [Positive/Negative]\\n\"\n",
    "        f\"- reason: [Reason explaining the emotional classification for the first behavior]\\n\"\n",
    "        f\"- behavior: [Second identified behavior in detail]\\n\"\n",
    "        f\"- emotional: [Positive/Negative]\\n\"\n",
    "        f\"- reason: [Reason explaining the emotional classification for the second behavior]\\n\"\n",
    "        f\"... (continue for all identified behaviors, emotions, and reasons)\\n\"\n",
    "        f\"<|eot_id|>\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 600, \n",
    "        \"do_sample\": True, \n",
    "        \"temperature\": 0.7, \n",
    "        \"top_k\": 100,  \n",
    "        \"top_p\": 0.9 \n",
    "    }\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "file_path = '/workspace/clean_data/processed_2409_servicesAustralia_linkdin_media_blind_screenreader_voiceOver_Pat_patsAudio.txt'  \n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "for context in lines:\n",
    "    context = context.strip() \n",
    "    answer = generate_answer(context)\n",
    "    print(f\"Input: {context}\")\n",
    "    print(f\"{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee8917-9fd7-490d-8513-dd23c856a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e588bf-dc66-4626-8f57-751c2977e41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
